{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc10b83",
   "metadata": {},
   "source": [
    "# Using AzureAIMemoryStore with LangGraph\n",
    "\n",
    "This notebook demonstrates how to use the `AzureAIMemoryStore` class from `langchain-azure-ai` to provide **persistent, AI-powered long-term memory** to LangGraph agents.\n",
    "\n",
    "Long-term memory allows agents to remember facts about users across conversations \u2014 for example, preferences, past interactions, or profile information. `AzureAIMemoryStore` implements LangGraph's `BaseStore` interface backed by Azure AI Foundry's memory stores service.\n",
    "\n",
    "## What you will learn\n",
    "\n",
    "1. How to create an `AzureAIMemoryStore` connected to Azure AI Foundry\n",
    "2. How to use it with a **standard LangGraph agent** (`create_react_agent`)\n",
    "3. How to use it with a **Microsoft Foundry agent** (`AgentServiceFactory`)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. A Microsoft Foundry project (new Foundry experience)\n",
    "2. A deployed chat model (e.g. `gpt-4.1`)\n",
    "3. A **memory store** created in your Foundry project ([documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/agents/how-to/memory-usage))\n",
    "4. Environment variables set:\n",
    "   - `AZURE_AI_PROJECT_ENDPOINT`\n",
    "   - `MODEL_DEPLOYMENT_NAME`\n",
    "   - `MEMORY_STORE_NAME`\n",
    "5. Install dependencies:\n",
    "\n",
    "```bash\n",
    "pip install \"langchain-azure-ai[preview]\" \"azure-ai-projects>=2.0.0b1\" langchain-openai langgraph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ce76c",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"] = \"https://<resource>.services.ai.azure.com/api/projects/<project>\"\n",
    "os.environ[\"MODEL_DEPLOYMENT_NAME\"] = \"gpt-4.1\"\n",
    "os.environ[\"MEMORY_STORE_NAME\"] = \"my-memory-store\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d841bb",
   "metadata": {},
   "source": [
    "If you run into issues, enable diagnostic logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e934408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable debug logging (optional)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2405896",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f82738",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "\n",
    "def pretty_print(response: dict) -> None:\n",
    "    \"\"\"Print all messages in a LangGraph response.\"\"\"\n",
    "    for m in response[\"messages\"]:\n",
    "        m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849e9c7",
   "metadata": {},
   "source": [
    "## Creating an AzureAIMemoryStore\n",
    "\n",
    "`AzureAIMemoryStore` wraps Azure AI Foundry's memory stores API behind LangGraph's `BaseStore` interface. Under the hood it calls `search_memories`, `begin_update_memories`, and `delete_scope` to persist and retrieve memories.\n",
    "\n",
    "You need an `AIProjectClient` (from the `azure-ai-projects` SDK V2) and the name of a memory store that already exists in your Foundry project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7994879a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from langchain_azure_ai.stores import AzureAIMemoryStore\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"],\n",
    "    credential=credential,\n",
    ")\n",
    "\n",
    "memory_store = AzureAIMemoryStore(\n",
    "    project_client=project_client,\n",
    "    memory_store_name=os.environ[\"MEMORY_STORE_NAME\"],\n",
    ")\n",
    "\n",
    "print(\"AzureAIMemoryStore created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d61580",
   "metadata": {},
   "source": [
    "### Direct store operations\n",
    "\n",
    "Before plugging the store into an agent, let's verify it works by storing and searching memories directly. The `put` method feeds conversation content to the Azure AI memory service, which extracts and persists relevant facts. The `search` method retrieves memories that are relevant to a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c69172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a memory \u2014 the service will extract relevant facts from the content\n",
    "memory_store.put(\n",
    "    namespace=(\"user\", \"alice\"),\n",
    "    key=\"conv-1\",\n",
    "    value={\"content\": \"My name is Alice and I love hiking in the mountains. I also enjoy Python programming.\"},\n",
    ")\n",
    "\n",
    "print(\"Memory stored for user alice.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b5604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for memories relevant to a query\n",
    "results = memory_store.search(\n",
    "    (\"user\", \"alice\"),\n",
    "    query=\"What are the user's hobbies?\",\n",
    ")\n",
    "\n",
    "print(f\"Found {len(results)} memories:\")\n",
    "for item in results:\n",
    "    print(f\"  [{item.key}] {item.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613e7b2",
   "metadata": {},
   "source": [
    "## Example 1 \u2014 Standard LangGraph agent with AzureAIMemoryStore\n",
    "\n",
    "In this example we use LangGraph's built-in `create_react_agent` with an Azure OpenAI chat model. The `store` parameter wires the `AzureAIMemoryStore` so that any node in the graph can read and write long-term memories.\n",
    "\n",
    "The agent has access to a `recall_memories` tool that searches the store for memories relevant to the current conversation, and a `save_memory` tool that persists new information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270fe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# Create chat model pointing at an Azure OpenAI deployment\n",
    "chat_model = AzureChatOpenAI(\n",
    "    azure_endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"],\n",
    "    azure_ad_token_provider=credential.get_token(\"https://cognitiveservices.azure.com/.default\").token,\n",
    "    api_version=\"2025-04-01-preview\",\n",
    "    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc8922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.config import get_store\n",
    "\n",
    "\n",
    "@tool\n",
    "def recall_memories(query: str, user_id: str) -> str:\n",
    "    \"\"\"Search the memory store for information relevant to the query for a given user.\n",
    "\n",
    "    Args:\n",
    "        query: The search query describing what information to look for.\n",
    "        user_id: The user whose memories to search.\n",
    "    \"\"\"\n",
    "    store = get_store()\n",
    "    results = store.search((\"user\", user_id), query=query, limit=5)\n",
    "    if not results:\n",
    "        return \"No relevant memories found.\"\n",
    "    return \"\\n\".join(f\"- {item.value.get('content', '')}\" for item in results)\n",
    "\n",
    "\n",
    "@tool\n",
    "def save_memory(content: str, user_id: str) -> str:\n",
    "    \"\"\"Save a piece of information to the memory store for a given user.\n",
    "\n",
    "    Args:\n",
    "        content: The information to remember.\n",
    "        user_id: The user to associate this memory with.\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "\n",
    "    store = get_store()\n",
    "    store.put((\"user\", user_id), str(uuid.uuid4()), {\"content\": content})\n",
    "    return f\"Memory saved for user {user_id}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d82069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent with the memory store\n",
    "langgraph_agent = create_react_agent(\n",
    "    model=chat_model,\n",
    "    tools=[recall_memories, save_memory],\n",
    "    prompt=\"You are a helpful assistant with access to a long-term memory store. \"\n",
    "           \"Use recall_memories to look up information about the user before answering. \"\n",
    "           \"Use save_memory to remember important facts the user shares.\",\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=memory_store,\n",
    ")\n",
    "\n",
    "print(\"LangGraph agent created with AzureAIMemoryStore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cda47e9",
   "metadata": {},
   "source": [
    "Let's have a conversation with the agent. We already stored Alice's preferences earlier, so the agent should be able to recall them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90ef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"session-1\"}}\n",
    "\n",
    "response = langgraph_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What do you know about Alice's hobbies? My user_id is alice.\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f526b184",
   "metadata": {},
   "source": [
    "Now let's tell the agent something new and verify it gets persisted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3521994",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = langgraph_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Please remember that Alice recently started learning Spanish. My user_id is alice.\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new session, the agent should recall all of Alice's information\n",
    "config_new = {\"configurable\": {\"thread_id\": \"session-2\"}}\n",
    "\n",
    "response = langgraph_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What do you know about Alice? My user_id is alice.\")]},\n",
    "    config=config_new,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c7e514",
   "metadata": {},
   "source": [
    "## Example 2 \u2014 Microsoft Foundry Agent with AzureAIMemoryStore\n",
    "\n",
    "In this example we use `AgentServiceFactory` from `langchain-azure-ai` to create a Foundry-backed agent. The `store` parameter passes our `AzureAIMemoryStore` to the compiled graph, so Foundry agent nodes can also access long-term memory.\n",
    "\n",
    "This pattern is useful when you want to combine Foundry's server-side agent execution with persistent, cross-session memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_azure_ai.agents.v2 import AgentServiceFactory\n",
    "\n",
    "factory = AgentServiceFactory(\n",
    "    project_endpoint=os.environ[\"AZURE_AI_PROJECT_ENDPOINT\"],\n",
    "    credential=credential,\n",
    ")\n",
    "\n",
    "print(\"AgentServiceFactory created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f8f88",
   "metadata": {},
   "source": [
    "### Creating the Foundry agent with tools and memory\n",
    "\n",
    "We define local tools for memory recall and saving, then create a Foundry agent that can use them. The `store` parameter connects our `AzureAIMemoryStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fc27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "foundry_agent = factory.create_prompt_agent(\n",
    "    name=\"memory-demo-agent\",\n",
    "    model=os.environ[\"MODEL_DEPLOYMENT_NAME\"],\n",
    "    instructions=(\n",
    "        \"You are a helpful assistant with access to a long-term memory store. \"\n",
    "        \"Use recall_memories to look up information about the user before answering. \"\n",
    "        \"Use save_memory to remember important facts the user shares.\"\n",
    "    ),\n",
    "    tools=[recall_memories, save_memory],\n",
    "    checkpointer=MemorySaver(),\n",
    "    store=memory_store,\n",
    ")\n",
    "\n",
    "print(f\"Foundry agent created: {factory.get_agents_id_from_graph(foundry_agent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f575c316",
   "metadata": {},
   "source": [
    "Let's visualise the agent graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3f3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(foundry_agent.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37f202b",
   "metadata": {},
   "source": [
    "### Conversing with the Foundry agent\n",
    "\n",
    "Since we already stored Alice's preferences, the Foundry agent should be able to recall them too \u2014 both agents share the same memory store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c6c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"foundry-session-1\"}}\n",
    "\n",
    "response = foundry_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What do you remember about Alice? My user_id is alice.\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dba4a65",
   "metadata": {},
   "source": [
    "Save new information via the Foundry agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = foundry_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Please remember that Alice prefers dark mode in all her apps. My user_id is alice.\")]},\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efaeacb",
   "metadata": {},
   "source": [
    "Verify the new memory is accessible from either agent \u2014 let's go back to the standard LangGraph agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f2eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_cross = {\"configurable\": {\"thread_id\": \"session-cross-check\"}}\n",
    "\n",
    "response = langgraph_agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me everything you know about Alice. My user_id is alice.\")]},\n",
    "    config=config_cross,\n",
    ")\n",
    "\n",
    "pretty_print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172ec8e",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Delete the Foundry agent and optionally remove the memories stored for Alice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e97a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Foundry agent from Azure AI Foundry\n",
    "factory.delete_agent(foundry_agent)\n",
    "print(\"Foundry agent deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbdc001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all memories for user alice\n",
    "memory_store.delete((\"user\", \"alice\"), key=\"*\")\n",
    "print(\"Memories cleaned up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c304aeb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you learned how to:\n",
    "\n",
    "- Create an `AzureAIMemoryStore` backed by Azure AI Foundry's memory stores service\n",
    "- Use it directly to store and search memories\n",
    "- Plug it into a **standard LangGraph agent** (`create_react_agent`) for long-term memory\n",
    "- Plug it into a **Microsoft Foundry agent** (`AgentServiceFactory.create_prompt_agent`) for long-term memory\n",
    "- Share the same memory store across different agent types\n",
    "\n",
    "The `AzureAIMemoryStore` implements LangGraph's `BaseStore` interface, so it can be used as a drop-in replacement for `InMemoryStore` in any LangGraph application that needs persistent, AI-powered memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-azure-ai-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}