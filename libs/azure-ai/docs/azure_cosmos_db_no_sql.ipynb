{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0243ae",
   "metadata": {},
   "source": [
    "# Azure Cosmos DB No SQL\n",
    "\n",
    "This notebook shows you how to leverage this integrated [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) to store documents in collections, create indicies and perform vector search queries using approximate nearest neighbor algorithms such as COS (cosine distance), L2 (Euclidean distance), and IP (inner product) to locate documents close to the query vectors. \n",
    "    \n",
    "Azure Cosmos DB is the database that powers OpenAI's ChatGPT service. It offers single-digit millisecond response times, automatic and instant scalability, along with guaranteed speed at any scale. \n",
    "\n",
    "Azure Cosmos DB for NoSQL now offers vector indexing and search in preview. This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data, but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching, as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the efficiency of vector-based operations.\n",
    "\n",
    "Please refer here for more details:\n",
    "- [Vector Search](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search)\n",
    "- [Full Text Search](https://learn.microsoft.com/en-us/azure/cosmos-db/gen-ai/full-text-search)\n",
    "- [Hybrid Search](https://learn.microsoft.com/en-us/azure/cosmos-db/gen-ai/hybrid-search)\n",
    "\n",
    "[Sign Up](https://azure.microsoft.com/en-us/free/) for lifetime free access to get started today."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad3c1e88",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "%pip install --upgrade --quiet azure-cosmos langchain-openai langchain-community"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c507b0e8",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "OPENAI_API_TYPE = \"azure\"\n",
    "OPENAI_API_VERSION = \"2024-07-01-preview\"\n",
    "OPENAI_API_BASE = \"\"\n",
    "OPENAI_EMBEDDINGS_MODEL_NAME = \"text-embedding-3-small\"\n",
    "OPENAI_EMBEDDINGS_MODEL_DEPLOYMENT = \"text-embedding-3-small\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa7101f64740fb76",
   "metadata": {},
   "source": [
    "## Insert Data"
   ]
  },
  {
   "cell_type": "code",
   "id": "8205cd27",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Load the PDF\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2303.08774.pdf\")\n",
    "data = loader.load()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d33cceb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:16:14.004058Z",
     "start_time": "2025-02-05T23:16:13.831919Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "docs = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a80f1c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:16:16.748113Z",
     "start_time": "2025-02-05T23:16:16.743928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='GPT-4 Technical Report\n",
      "OpenAI∗\n",
      "Abstract\n",
      "We report the development of GPT-4, a large-scale, multimodal model which can\n",
      "accept image and text inputs and produce text outputs. While less capable than\n",
      "humans in many real-world scenarios, GPT-4 exhibits human-level performance\n",
      "on various professional and academic benchmarks, including passing a simulated\n",
      "bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\n",
      "based model pre-trained to predict the next token in a document. The post-training\n",
      "alignment process results in improved performance on measures of factuality and\n",
      "adherence to desired behavior. A core component of this project was developing\n",
      "infrastructure and optimization methods that behave predictably across a wide\n",
      "range of scales. This allowed us to accurately predict some aspects of GPT-4’s\n",
      "performance based on models trained with no more than 1/1,000th the compute of\n",
      "GPT-4.\n",
      "1 Introduction' metadata={'source': 'https://arxiv.org/pdf/2303.08774.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f13e237e91052",
   "metadata": {},
   "source": [
    "## Creating AzureCosmosDB NoSQL Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c72ccc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:16:20.511770Z",
     "start_time": "2025-02-05T23:16:20.506546Z"
    }
   },
   "outputs": [],
   "source": [
    "indexing_policy = {\n",
    "    \"indexingMode\": \"consistent\",\n",
    "    \"includedPaths\": [{\"path\": \"/*\"}],\n",
    "    \"excludedPaths\": [{\"path\": '/\"_etag\"/?'}],\n",
    "    \"vectorIndexes\": [{\"path\": \"/embedding\", \"type\": \"diskANN\"}],\n",
    "    \"fullTextIndexes\": [{\"path\": \"/text\"}],\n",
    "}\n",
    "\n",
    "vector_embedding_policy = {\n",
    "    \"vectorEmbeddings\": [\n",
    "        {\n",
    "            \"path\": \"/embedding\",\n",
    "            \"dataType\": \"float32\",\n",
    "            \"distanceFunction\": \"cosine\",\n",
    "            \"dimensions\": 1536,\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "full_text_policy = {\n",
    "    \"defaultLanguage\": \"en-US\",\n",
    "    \"fullTextPaths\": [{\"path\": \"/text\", \"language\": \"en-US\"}],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ebad8ef01a6c04f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:51:13.990817Z",
     "start_time": "2025-02-05T23:46:48.038840Z"
    }
   },
   "outputs": [],
   "source": [
    "from azure.cosmos import CosmosClient, PartitionKey\n",
    "from langchain_community.vectorstores.azure_cosmos_db_no_sql import (\n",
    "    AzureCosmosDBNoSqlVectorSearch,\n",
    ")\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from pydantic import SecretStr\n",
    "\n",
    "HOST = \"AZURE_COSMOS_DB_ENDPOINT\"\n",
    "KEY = \"AZURE_COSMOS_DB_KEY\"\n",
    "\n",
    "cosmos_client = CosmosClient(HOST, KEY)\n",
    "database_name = \"langchain_python_db_notebook\"\n",
    "container_name = \"langchain_python_container\"\n",
    "partition_key = PartitionKey(path=\"/id\")\n",
    "cosmos_container_properties = {\"partition_key\": partition_key}\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(\n",
    "    deployment=\"smart-agent-embedding-ada\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    chunk_size=1,\n",
    "    openai_api_key=SecretStr(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "\n",
    "# insert the documents in AzureCosmosDBNoSql with their embedding\n",
    "vector_search = AzureCosmosDBNoSqlVectorSearch.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=openai_embeddings,\n",
    "    cosmos_client=cosmos_client,\n",
    "    database_name=database_name,\n",
    "    container_name=container_name,\n",
    "    vector_embedding_policy=vector_embedding_policy,\n",
    "    full_text_policy=full_text_policy,\n",
    "    indexing_policy=indexing_policy,\n",
    "    cosmos_container_properties={\"partition_key\": partition_key},\n",
    "    cosmos_database_properties={},\n",
    "    vector_search_fields={\"text_field\": \"text\", \"embedding_field\": \"embedding\"},\n",
    "    full_text_search_enabled=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ff6adb7c8ad48",
   "metadata": {},
   "source": "## Vector Search"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf7a2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:26:43.922905Z",
     "start_time": "2025-02-05T23:26:42.569254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance based on models trained with no more than 1/1,000th the compute of\n",
      "GPT-4.\n",
      "1 Introduction\n",
      "This technical report presents GPT-4, a large multimodal model capable of processing image and\n",
      "text inputs and producing text outputs. Such models are an important area of study as they have the\n",
      "potential to be used in a wide range of applications, such as dialogue systems, text summarization,\n",
      "and machine translation. As such, they have been the subject of substantial interest and progress in\n",
      "recent years [1–34].\n",
      "One of the main goals of developing such models is to improve their ability to understand and generate\n",
      "natural language text, particularly in more complex and nuanced scenarios. To test its capabilities\n",
      "in such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\n",
      "these evaluations it performs quite well and often outscores the vast majority of human test takers.\n"
     ]
    }
   ],
   "source": [
    "# Perform a similarity search between the embedding of the query and the embeddings of the documents\n",
    "import json\n",
    "\n",
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "results = vector_search.similarity_search(query)\n",
    "\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d77484c71fe192",
   "metadata": {},
   "source": "## Vector Search with Score"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6371c107306cd76d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:26:48.421519Z",
     "start_time": "2025-02-05T23:26:48.058929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"5a9a248f-6885-4e07-8321-e416ecd01556\"},\"page_content\":\"performance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\",\"type\":\"Document\"}\n",
      "Score 1:  0.642735520879037\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":1,\"id\":\"64788ac7-2665-4987-994f-0086d701c909\"},\"page_content\":\"safety considerations above against the scientific value of further transparency.\\n3 Predictable Scaling\\nA large focus of the GPT-4 project was building a deep learning stack that scales predictably. The\\nprimary reason is that for very large training runs like GPT-4, it is not feasible to do extensive\\nmodel-specific tuning. To address this, we developed infrastructure and optimization methods that\\nhave very predictable behavior across multiple scales. These improvements allowed us to reliably\\npredict some aspects of the performance of GPT-4 from smaller models trained using 1,000×–\\n10,000×less compute.\\n3.1 Loss Prediction\\nThe final loss of properly-trained large language models is thought to be well approximated by power\\nlaws in the amount of compute used to train the model [41, 42, 2, 14, 15].\\nTo verify the scalability of our optimization infrastructure, we predicted GPT-4’s final loss on our\",\"type\":\"Document\"}\n",
      "Score 2:  0.6270494557311032\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"9846e748-87fc-4f17-a8cb-9c11699d6158\"},\"page_content\":\"GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\",\"type\":\"Document\"}\n",
      "Score 3:  0.6231760505455314\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":28,\"id\":\"ec2d87d9-6cf4-4f42-8759-1f55c31ecd2b\"},\"page_content\":\"overall GPT-4 training budget. When mixing in data from these math benchmarks, a portion of the\\ntraining data was held back, so each individual training example may or may not have been seen by\\nGPT-4 during training.\\nWe conducted contamination checking to verify the test set for GSM-8K is not included in the training\\nset (see Appendix D). We recommend interpreting the performance results reported for GPT-4\\nGSM-8K in Table 2 as something in-between true few-shot transfer and full benchmark-specific\\ntuning.\\nF Multilingual MMLU\\nWe translated all questions and answers from MMLU [ 49] using Azure Translate. We used an\\nexternal model to perform the translation, instead of relying on GPT-4 itself, in case the model had\\nunrepresentative performance for its own translations. We selected a range of languages that cover\\ndifferent geographic regions and scripts, we show an example question taken from the astronomy\",\"type\":\"Document\"}\n",
      "Score 4:  0.5950017702893886\n",
      "\n",
      "\n",
      "Result 5:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":2,\"id\":\"ea46ce2e-4c73-4a3f-8098-8ec6bb664f12\"},\"page_content\":\"Observed\\nPrediction\\ngpt-4\\n100p 10n 1µ 100µ 0.01 1\\nCompute1.02.03.04.05.06.0Bits per wordOpenAI codebase next word predictionFigure 1. Performance of GPT-4 and smaller models. The metric is final loss on a dataset derived\\nfrom our internal codebase. This is a convenient, large dataset of code tokens which is not contained in\\nthe training set. We chose to look at loss because it tends to be less noisy than other measures across\\ndifferent amounts of training compute. A power law fit to the smaller models (excluding GPT-4) is\\nshown as the dotted line; this fit accurately predicts GPT-4’s final loss. The x-axis is training compute\\nnormalized so that GPT-4 is 1.\\nObserved\\nPrediction\\ngpt-4\\n1µ 10µ 100µ 0.001 0.01 0.1 1\\nCompute012345– Mean Log Pass RateCapability prediction on 23 coding problems\\nFigure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\",\"type\":\"Document\"}\n",
      "Score 5:  0.586029243650397\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(f\"Score {i+1}: \", results[i][1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ffb492375192d",
   "metadata": {},
   "source": "## Vector Search with filtering"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "461c6ac2ba3cee2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:56:14.465211Z",
     "start_time": "2025-02-05T23:56:12.448642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"4b0034fa-0d0e-46b3-9385-0582511eb28f\"},\"page_content\":\"performance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\",\"type\":\"Document\"}\n",
      "Score 1:  0.642735520879037\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"87ec78d5-26e9-4eae-afd3-07935f706230\"},\"page_content\":\"GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\",\"type\":\"Document\"}\n",
      "Score 2:  0.6231760505455314\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"83fa9b0b-d25d-4b7f-bf79-0d39bf2a1033\"},\"page_content\":\"these evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\\nand most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\\nOn the MMLU benchmark [ 35,36], an English-language suite of multiple-choice questions covering\\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\\nmodel capability results, as well as model safety improvements and results, in more detail in later\\nsections.\",\"type\":\"Document\"}\n",
      "Score 3:  0.5690217547521872\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"430bc9df-336b-4002-b67c-df76381131ad\"},\"page_content\":\"model capability results, as well as model safety improvements and results, in more detail in later\\nsections.\\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [ 1,37,38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\\\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.comarXiv:2303.08774v6  [cs.CL]  4 Mar 2024\",\"type\":\"Document\"}\n",
      "Score 4:  0.5253629308670477\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores.azure_cosmos_db_no_sql import (\n",
    "    Condition,\n",
    "    PreFilter,\n",
    ")\n",
    "\n",
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "\n",
    "pre_filter = PreFilter(\n",
    "    conditions=[\n",
    "        Condition(property=\"metadata.page\", operator=\"$eq\", value=0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    pre_filter=pre_filter,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(f\"Score {i+1}: \", results[i][1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7b4932ed5f42a",
   "metadata": {},
   "source": "## Full Text Search"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a58a4cbcc160a0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:57:54.695208Z",
     "start_time": "2025-02-05T23:57:54.451086Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"87ec78d5-26e9-4eae-afd3-07935f706230\"},\"page_content\":\"GPT-4 Technical Report\\nOpenAI∗\\nAbstract\\nWe report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance\\non various professional and academic benchmarks, including passing a simulated\\nbar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-\\nbased model pre-trained to predict the next token in a document. The post-training\\nalignment process results in improved performance on measures of factuality and\\nadherence to desired behavior. A core component of this project was developing\\ninfrastructure and optimization methods that behave predictably across a wide\\nrange of scales. This allowed us to accurately predict some aspects of GPT-4’s\\nperformance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"4b0034fa-0d0e-46b3-9385-0582511eb28f\"},\"page_content\":\"performance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"83fa9b0b-d25d-4b7f-bf79-0d39bf2a1033\"},\"page_content\":\"these evaluations it performs quite well and often outscores the vast majority of human test takers.\\nFor example, on a simulated bar exam, GPT-4 achieves a score that falls in the top 10% of test takers.\\nThis contrasts with GPT-3.5, which scores in the bottom 10%.\\nOn a suite of traditional NLP benchmarks, GPT-4 outperforms both previous large language models\\nand most state-of-the-art systems (which often have benchmark-specific training or hand-engineering).\\nOn the MMLU benchmark [ 35,36], an English-language suite of multiple-choice questions covering\\n57 subjects, GPT-4 not only outperforms existing models by a considerable margin in English, but\\nalso demonstrates strong performance in other languages. On translated variants of MMLU, GPT-4\\nsurpasses the English-language state-of-the-art in 24 of 26 languages considered. We discuss these\\nmodel capability results, as well as model safety improvements and results, in more detail in later\\nsections.\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":0,\"id\":\"430bc9df-336b-4002-b67c-df76381131ad\"},\"page_content\":\"model capability results, as well as model safety improvements and results, in more detail in later\\nsections.\\nThis report also discusses a key challenge of the project, developing deep learning infrastructure and\\noptimization methods that behave predictably across a wide range of scales. This allowed us to make\\npredictions about the expected performance of GPT-4 (based on small runs trained in similar ways)\\nthat were tested against the final run to increase confidence in our training.\\nDespite its capabilities, GPT-4 has similar limitations to earlier GPT models [ 1,37,38]: it is not fully\\nreliable (e.g. can suffer from “hallucinations”), has a limited context window, and does not learn\\n∗Please cite this work as “OpenAI (2023)\\\". Full authorship contribution statements appear at the end of the\\ndocument. Correspondence regarding this technical report can be sent to gpt4-report@openai.comarXiv:2303.08774v6  [cs.CL]  4 Mar 2024\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 5:  {\"id\":null,\"metadata\":{\"source\":\"https://arxiv.org/pdf/2303.08774.pdf\",\"page\":1,\"id\":\"fa3aa45c-509b-4577-b86f-fff102df69ec\"},\"page_content\":\"from experience. Care should be taken when using the outputs of GPT-4, particularly in contexts\\nwhere reliability is important.\\nGPT-4’s capabilities and limitations create significant and novel safety challenges, and we believe\\ncareful study of these challenges is an important area of research given the potential societal impact.\\nThis report includes an extensive system card (after the Appendix) describing some of the risks we\\nforesee around bias, disinformation, over-reliance, privacy, cybersecurity, proliferation, and more.\\nIt also describes interventions we made to mitigate potential harms from the deployment of GPT-4,\\nincluding adversarial testing with domain experts, and a model-assisted safety pipeline.\\n2 Scope and Limitations of this Technical Report\\nThis report focuses on the capabilities, limitations, and safety properties of GPT-4. GPT-4 is a\\nTransformer-style model [ 39] pre-trained to predict the next token in a document, using both publicly\",\"type\":\"Document\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "pre_filter = PreFilter(\n",
    "    conditions=[\n",
    "        Condition(\n",
    "            property=\"text\",\n",
    "            operator=\"$full_text_contains_any\",\n",
    "            value=\"What were the compute requirements for training GPT 4\",\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    query_type=\"full_text_search\",\n",
    "    pre_filter=pre_filter,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2831548f1fb4eb90",
   "metadata": {},
   "source": "## Full Text Search BM 25 Ranking"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73dacc61ce8bf7a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:59:15.068100Z",
     "start_time": "2025-02-05T23:59:14.466167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"id\":\"f81d994b-bd4e-4471-905f-841ac529584d\"},\"page_content\":\"the HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"id\":\"b8117761-b5ec-473d-a818-dd5f7dda75ac\"},\"page_content\":\"safety considerations above against the scientific value of further transparency.\\n3 Predictable Scaling\\nA large focus of the GPT-4 project was building a deep learning stack that scales predictably. The\\nprimary reason is that for very large training runs like GPT-4, it is not feasible to do extensive\\nmodel-specific tuning. To address this, we developed infrastructure and optimization methods that\\nhave very predictable behavior across multiple scales. These improvements allowed us to reliably\\npredict some aspects of the performance of GPT-4 from smaller models trained using 1,000×–\\n10,000×less compute.\\n3.1 Loss Prediction\\nThe final loss of properly-trained large language models is thought to be well approximated by power\\nlaws in the amount of compute used to train the model [41, 42, 2, 14, 15].\\nTo verify the scalability of our optimization infrastructure, we predicted GPT-4’s final loss on our\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"id\":\"96549f60-a72e-42fe-9dec-772cfe0ddd32\"},\"page_content\":\"Observed\\nPrediction\\ngpt-4\\n100p 10n 1µ 100µ 0.01 1\\nCompute1.02.03.04.05.06.0Bits per wordOpenAI codebase next word predictionFigure 1. Performance of GPT-4 and smaller models. The metric is final loss on a dataset derived\\nfrom our internal codebase. This is a convenient, large dataset of code tokens which is not contained in\\nthe training set. We chose to look at loss because it tends to be less noisy than other measures across\\ndifferent amounts of training compute. A power law fit to the smaller models (excluding GPT-4) is\\nshown as the dotted line; this fit accurately predicts GPT-4’s final loss. The x-axis is training compute\\nnormalized so that GPT-4 is 1.\\nObserved\\nPrediction\\ngpt-4\\n1µ 10µ 100µ 0.001 0.01 0.1 1\\nCompute012345– Mean Log Pass RateCapability prediction on 23 coding problems\\nFigure 2. Performance of GPT-4 and smaller models. The metric is mean log pass rate on a subset of\\nthe HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"id\":\"98f12229-18ee-4407-96f8-b64e3c99aac5\"},\"page_content\":\"which measures the ability to synthesize Python functions of varying complexity. We successfully\\npredicted the pass rate on a subset of the HumanEval dataset by extrapolating from models trained\\nwith at most 1,000×less compute (Figure 2).\\nFor an individual problem in HumanEval, performance may occasionally worsen with scale. Despite\\nthese challenges, we find an approximate power law relationship −EP[log(pass _rate(C))] = α∗C−k\\n2In addition to the accompanying system card, OpenAI will soon publish additional thoughts on the social\\nand economic implications of AI systems, including the need for effective regulation.\\n2\",\"type\":\"Document\"}\n",
      "\n",
      "\n",
      "Result 5:  {\"id\":null,\"metadata\":{\"id\":\"8c5a8136-9944-44f7-90d6-575b922ff5cf\"},\"page_content\":\"Unsupervised Multitask Learners,” 2019.\\n[23]G. C. Bowker and S. L. Star, Sorting Things Out . MIT Press, Aug. 2000.\\n[24]L. Weidinger, J. Uesato, M. Rauh, C. Griﬃn, P.-S. Huang, J. Mellor, A. Glaese, M. Cheng,\\nB. Balle, A. Kasirzadeh, C. Biles, S. Brown, Z. Kenton, W. Hawkins, T. Stepleton, A. Birhane,\\nL. A. Hendricks, L. Rimell, W. Isaac, J. Haas, S. Legassick, G. Irving, and I. Gabriel, “Taxonomy\\nof Risks posed by Language Models,” in 2022 ACM Conference on Fairness, Accountability,\\nand Transparency , FAccT ’22, (New York, NY, USA), pp. 214–229, Association for Computing\\nMachinery, June 2022.\\n72\",\"type\":\"Document\"}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "\n",
    "full_text_rank_filter = [\n",
    "    {\n",
    "        \"search_field\": \"text\",\n",
    "        \"search_text\": \"What were the compute requirements for training GPT 4\",\n",
    "    }\n",
    "]\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    query_type=\"full_text_ranking\",\n",
    "    full_text_rank_filter=full_text_rank_filter,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e4eb20a141031",
   "metadata": {},
   "source": "## Hybrid Search"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e1f7c17b02579b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T23:59:41.709229Z",
     "start_time": "2025-02-05T23:59:40.112068Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"id\":\"3ae8615a-5fd3-4543-8d94-b01687904e02\"},\"page_content\":\"Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98\",\"type\":\"Document\"}\n",
      "Score 1:  0.5545045822126439\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"id\":\"f81d994b-bd4e-4471-905f-841ac529584d\"},\"page_content\":\"the HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3\",\"type\":\"Document\"}\n",
      "Score 2:  0.5529193759066282\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"id\":\"b8117761-b5ec-473d-a818-dd5f7dda75ac\"},\"page_content\":\"safety considerations above against the scientific value of further transparency.\\n3 Predictable Scaling\\nA large focus of the GPT-4 project was building a deep learning stack that scales predictably. The\\nprimary reason is that for very large training runs like GPT-4, it is not feasible to do extensive\\nmodel-specific tuning. To address this, we developed infrastructure and optimization methods that\\nhave very predictable behavior across multiple scales. These improvements allowed us to reliably\\npredict some aspects of the performance of GPT-4 from smaller models trained using 1,000×–\\n10,000×less compute.\\n3.1 Loss Prediction\\nThe final loss of properly-trained large language models is thought to be well approximated by power\\nlaws in the amount of compute used to train the model [41, 42, 2, 14, 15].\\nTo verify the scalability of our optimization infrastructure, we predicted GPT-4’s final loss on our\",\"type\":\"Document\"}\n",
      "Score 3:  0.6270494557311032\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"id\":\"4b0034fa-0d0e-46b3-9385-0582511eb28f\"},\"page_content\":\"performance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\",\"type\":\"Document\"}\n",
      "Score 4:  0.642735520879037\n",
      "\n",
      "\n",
      "Result 5:  {\"id\":null,\"metadata\":{\"id\":\"89751c93-55eb-4497-ac51-64b07368fab9\"},\"page_content\":\"Preliminary results on a narrow set of academic vision benchmarks can be found in the GPT-4 blog\\npost [ 65]. We plan to release more information about GPT-4’s visual capabilities in follow-up work.\\n8\",\"type\":\"Document\"}\n",
      "Score 5:  0.5310937141136488\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "\n",
    "full_text_rank_filter = [\n",
    "    {\n",
    "        \"search_field\": \"text\",\n",
    "        \"search_text\": \"What were the compute requirements for training GPT 4\",\n",
    "    }\n",
    "]\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    query_type=\"hybrid\",\n",
    "    full_text_rank_filter=full_text_rank_filter,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(f\"Score {i+1}: \", results[i][1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178119f59653565",
   "metadata": {},
   "source": "## Hybrid Search with filtering"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a644e56095c897fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T00:01:16.329006Z",
     "start_time": "2025-02-06T00:01:14.417795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1:  {\"id\":null,\"metadata\":{\"id\":\"3ae8615a-5fd3-4543-8d94-b01687904e02\"},\"page_content\":\"Figure 11: Results on IF evaluations across GPT3.5, GPT3.5-Turbo, GPT-4-launch\\n98\",\"type\":\"Document\"}\n",
      "Score 1:  0.5545045822126439\n",
      "\n",
      "\n",
      "Result 2:  {\"id\":null,\"metadata\":{\"id\":\"f81d994b-bd4e-4471-905f-841ac529584d\"},\"page_content\":\"the HumanEval dataset. A power law fit to the smaller models (excluding GPT-4) is shown as the dotted\\nline; this fit accurately predicts GPT-4’s performance. The x-axis is training compute normalized so that\\nGPT-4 is 1.\\n3\",\"type\":\"Document\"}\n",
      "Score 2:  0.5529193759066282\n",
      "\n",
      "\n",
      "Result 3:  {\"id\":null,\"metadata\":{\"id\":\"b8117761-b5ec-473d-a818-dd5f7dda75ac\"},\"page_content\":\"safety considerations above against the scientific value of further transparency.\\n3 Predictable Scaling\\nA large focus of the GPT-4 project was building a deep learning stack that scales predictably. The\\nprimary reason is that for very large training runs like GPT-4, it is not feasible to do extensive\\nmodel-specific tuning. To address this, we developed infrastructure and optimization methods that\\nhave very predictable behavior across multiple scales. These improvements allowed us to reliably\\npredict some aspects of the performance of GPT-4 from smaller models trained using 1,000×–\\n10,000×less compute.\\n3.1 Loss Prediction\\nThe final loss of properly-trained large language models is thought to be well approximated by power\\nlaws in the amount of compute used to train the model [41, 42, 2, 14, 15].\\nTo verify the scalability of our optimization infrastructure, we predicted GPT-4’s final loss on our\",\"type\":\"Document\"}\n",
      "Score 3:  0.6270494557311032\n",
      "\n",
      "\n",
      "Result 4:  {\"id\":null,\"metadata\":{\"id\":\"4b0034fa-0d0e-46b3-9385-0582511eb28f\"},\"page_content\":\"performance based on models trained with no more than 1/1,000th the compute of\\nGPT-4.\\n1 Introduction\\nThis technical report presents GPT-4, a large multimodal model capable of processing image and\\ntext inputs and producing text outputs. Such models are an important area of study as they have the\\npotential to be used in a wide range of applications, such as dialogue systems, text summarization,\\nand machine translation. As such, they have been the subject of substantial interest and progress in\\nrecent years [1–34].\\nOne of the main goals of developing such models is to improve their ability to understand and generate\\nnatural language text, particularly in more complex and nuanced scenarios. To test its capabilities\\nin such scenarios, GPT-4 was evaluated on a variety of exams originally designed for humans. In\\nthese evaluations it performs quite well and often outscores the vast majority of human test takers.\",\"type\":\"Document\"}\n",
      "Score 4:  0.642735520879037\n",
      "\n",
      "\n",
      "Result 5:  {\"id\":null,\"metadata\":{\"id\":\"89751c93-55eb-4497-ac51-64b07368fab9\"},\"page_content\":\"Preliminary results on a narrow set of academic vision benchmarks can be found in the GPT-4 blog\\npost [ 65]. We plan to release more information about GPT-4’s visual capabilities in follow-up work.\\n8\",\"type\":\"Document\"}\n",
      "Score 5:  0.5310937141136488\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What were the compute requirements for training GPT 4\"\n",
    "\n",
    "pre_filter = PreFilter(\n",
    "    conditions=[\n",
    "        Condition(\n",
    "            property=\"text\",\n",
    "            operator=\"$full_text_contains_any\",\n",
    "            value=\"What were the compute requirements for training GPT 4\",\n",
    "        ),\n",
    "        Condition(property=\"metadata.page\", operator=\"$eq\", value=0),\n",
    "    ],\n",
    "    logical_operator=\"$and\",\n",
    ")\n",
    "\n",
    "full_text_rank_filter = [\n",
    "    {\n",
    "        \"search_field\": \"text\",\n",
    "        \"search_text\": \"What were the compute requirements for training GPT 4\",\n",
    "    }\n",
    "]\n",
    "results = vector_search.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    query_type=\"hybrid\",\n",
    "    full_text_rank_filter=full_text_rank_filter,\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i in range(0, len(results)):\n",
    "    print(f\"Result {i+1}: \", results[i][0].json())\n",
    "    print(f\"Score {i+1}: \", results[i][1])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de996623625ab622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
